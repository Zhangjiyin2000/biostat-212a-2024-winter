---
title: "Biostat 212a Homework 5"
subtitle: "Due Mar 11, 2024 @ 11:59PM"
author: "Jiyin (Jenny) Zhang, UID: 606331859"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 9.7.1 (10pts)

This problem involves hyperplanes in two dimensions.

__(a)__ Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set of points for 
which $1+3X_1 −X_2 > 0$, as well as the set of points for which 
$1 + 3X_1 − X_2 < 0$.

__(b)__ On the same plot, sketch the hyperplane $−2 + X_1 + 2X_2 = 0$. Indicate 
the set of points for which $−2 + X_1 + 2X_2 > 0$, as well as the set of points 
for which $−2+X_1+2X_2 <0$.

**Answer:**

Plotting the line:

$$
1 + 3X_1 − X_2 = 0 \Leftrightarrow X_2 = 1 + 3X_1

$$
Rearranging for $X_2$ makes it clear which side of the hyperplane is which:

$$
1 + 3X_1 − X_2 > 0 \Leftrightarrow X_2 < 1 + 3X_1 \\

1 + 3X_1 − X_2 < 0 \Leftrightarrow X_2 > 1 + 3X_1

$$

Plotting another line:

$$
−2 + X_1 + 2X_2 = 0 \Leftrightarrow X_2 = 1 - \frac{1}{2}X_1

$$
Rearranging for $X_2$ makes it clear which side of the hyperplane is which:

$$
−2 + X_1 + 2X_2 > 0 \Leftrightarrow  X_2 > 1 - \frac{1}{2}X_1 \\

−2 + X_1 + 2X_2 < 0 \Leftrightarrow X_2 < 1 - \frac{1}{2}X_1

$$

```{r}
library(ggplot2)

# line in a)
df <- data.frame(x1 = -10:10, x2 = 1 + 3 * (-10:10))
# line in b)
df2 <- data.frame(x1 = -10:10, x2 = 1 - 1/2 * (-10:10))

ggplot(df, aes(x = x1, y = x2)) +
  geom_line(color = "blue") + 
  # Add math expressions
  geom_text(aes(x = 0, y = -20), label ="paste('1 + 3', italic(X[1]), ' - ', italic(X[2]), ' > 0')", color = "blue", parse = TRUE) + 
  geom_text(aes(x = 0, y = 20), label = "paste('1 + 3', italic(X[1]), ' - ', italic(X[2]), ' < 0')", color = "blue", parse = TRUE) + 
  theme_minimal() +
  labs(x = "X1", y = "X2") +
  geom_line(data = df2, aes(x = x1, y = x2), color = "red") +
  geom_text(data = df2, aes(x = -5, y = 0), label = "paste('-2 + ', italic(X[1]), ' + 2', italic(X[2]), ' < 0')", color = "red", parse = TRUE) +
  geom_text(data = df2, aes(x = 7, y = 0), label = "paste('-2 + ', italic(X[1]), ' + 2', italic(X[2]), ' > 0')", color = "red", parse = TRUE) 
  
```




## ISL Exercise 9.7.2 (10pts)

We have seen that in p = 2 dimensions, a linear decision boundary takes the form 
$\beta_0 +\beta_1X_1 +\beta_2X_2 = 0$. We now investigate a non-linear decision 
boundary.

__(a)__ Sketch the curve

$$
(1+X_1)^2 + (2−X_2)^2 = 4
$$

**Answer:**

```{r}
library(ggplot2)
library(dplyr)
# install.packages("ggforce")
library(ggforce)
# install.packages("latex2exp")
library(latex2exp)
ggplot() + 
  geom_circle(data = data.frame(X1 = -1, X2 = 2, r = 2), 
              aes(x0 = X1, y0 = X2, r = r), col = "mediumseagreen", size = 1) + 
  scale_x_continuous(expand = c(0.01,0.01), limits = c(-3.5, 1.5)) + 
  scale_y_continuous(expand = c(0.01,0.01), limits = c(-0.5, 4.5)) + 
  labs(title = TeX(r'(Curve Plot: $(1 + X_1)^2 + (2 - X_2)^2 = 4$)'), 
       x = TeX(r'($X_1$)'), 
       y = TeX(r'($X_2$)')) + 
  coord_fixed() +
  geom_point(data = data.frame(X1 = -1, X2 = 2), aes(x = X1, y = X2), col = "red", size = 3)

```


__(b)__ On your sketch, indicate the set of points for which

$$
(1+X_1)^2 + (2−X_2)^2 > 4

$$

as well as the set of points for which

$$
(1+X_1)^2 + (2−X_2)^2 < 4

$$

**Answer:**

```{r}

grid <- expand.grid(X1 = seq(-3.5, 1.5, length.out = 200), 
                    X2 = seq(-0.5, 4.5, length.out = 200)) %>%
  mutate(region = case_when((1 + X1)^2 + (2 - X2)^2 > 4 ~ "gt 4", 
                            TRUE ~ "le 4"))

ggplot() + 
  geom_tile(data = grid, aes(x = X1, y = X2, fill = region), alpha = 0.5) + 
  geom_circle(data = data.frame(X1 = -1, X2 = 2, r = 2), 
              aes(x0 = X1, y0 = X2, r = r), col = "mediumseagreen", size = 1) + 
  annotate("text", x = -1, y = 2, label = TeX("$(1 + X_1)^2 + (2 - X_2)^2 \\leq 4$", output = "character"),
           hjust = 0.5, size = 3, parse = TRUE, col = "red") + 
  annotate("text", x = 0.5, y = 0, label = TeX("$(1 + X_1)^2 + (2 - X_2)^2 > 4$", output = "character"),
           hjust = 0.5, size = 3, parse = TRUE, col = "blue") + 
  scale_x_continuous(expand = c(0.01,0.01), limits = c(-3.5, 1.5)) + 
  scale_y_continuous(expand = c(0.01,0.01), limits = c(-0.5, 4.5)) + 
    scale_fill_manual(values = c("lightblue", "red")) +
  theme(legend.position = "none") + 
  labs(title = TeX(r'(Curve Plot: $(1 + X_1)^2 + (2 - X_2)^2 = 4$)'), 
       x = TeX(r'($X_1$)'), 
       y = TeX(r'($X_2$)')) + 
  coord_fixed()

```

__(c)__ Suppose that a classifier assigns an observation to the blue class if

$$
(1+X_1)^2 + (2−X_2)^2 > 4

$$

and to the red class otherwise. To what class is the observation $(0, 0)$ 
classified? $(-1, 1)$? $(2, 2)$? $(3, 8)$?

**Answer:**


```{r}
new_obs <- data.frame(X1 = c(0, -1, 2, 3), X2 = c(0, 1, 2, 8)) %>%
  mutate(region = case_when((1 + X1)^2 + (2 - X2)^2 > 4 ~ "gt 4", 
                            TRUE ~ "le 4"))


grid <- expand.grid(X1 = seq(-3.5, 3.5, length.out = 200), 
                    X2 = seq(-0.5, 8.5, length.out = 200)) %>%
  mutate(region = case_when((1 + X1)^2 + (2 - X2)^2 > 4 ~ "gt 4", 
                            TRUE ~ "le 4"))

ggplot() + 
  geom_tile(data = grid, aes(x = X1, y = X2, fill = region), alpha = 0.5) + 
  geom_circle(data = data.frame(X1 = -1, X2 = 2, r = 2), 
              aes(x0 = X1, y0 = X2, r = r), col = "mediumseagreen", size = 1) + 
  geom_point(data = new_obs, aes(x = X1, y = X2, col = region)) + 
  scale_x_continuous(expand = c(0.01,0.01), limits = c(-3.5, 3.5)) + 
  scale_y_continuous(expand = c(0.01,0.01), limits = c(-0.5, 8.5)) + 
  scale_fill_manual(values = c("lightblue", "red")) +
  scale_color_manual(values = c("blue", "red")) +
  theme(legend.position = "none") + 
  labs(title = TeX(r'(Classifier Plot: $f(X) = (1 + X_1)^2 + (2 - X_2)^2 - 4$)'), 
       x = TeX(r'($X_1$)'), 
       y = TeX(r'($X_2$)')) + 
    coord_fixed()

```
Base on the plot, we can see that:

- $(0, 0)$: The point is outside the circle, so it is classified as blue.
- $(-1, 1)$: The point is inside the circle, so it is classified as red.
- $(2, 2)$: The point is outside the circle, so it is classified as blue.
- $(3, 8)$: The point is outside the circle, so it is classified as blue.

__(d)__ Argue that while the decision boundary in (c) is not linear in terms of 
X1 and X2, it is linear in terms of $X_1$, $X_1^2$, $X_2$, and $X_2^2$.

**Answer:**

The decision boundary is the boundary described by $f(X)=0$:

$$
\begin{align*}
f(X) &= 0 \\
\Rightarrow f(X) &= (1 + X_1)^2 + (2 - X_2)^2 - 4  \\
&= X_1^2 + 2X_1 + 1 + X_2^2 - 4X_2 + 4 - 4 \\
&= 1 + X_1^2 + 2X_1 + X_2^2 - 4X_2 \\
&= \beta_0 + \beta_1Z_1 + \beta_2Z_1 + \beta_3Z + \beta_4Z_2
\end{align*}
$$

Therefore, the decision boundary is linear in terms of $X_1$, $X_1^2$, $X_2$, 
and $X_2^2$.

## Support vector machines (SVMs) on the `Carseats` data set (30pts)

Follow the machine learning workflow to train support vector classifier (same as SVM with linear kernel), SVM with polynomial kernel (tune the degree and regularization parameter $C$), and SVM with radial kernel (tune the scale parameter $\gamma$ and regularization parameter $C$) for classifying `Sales<=8` versus `Sales>8`. Use the same seed as in your HW4 for the initial test/train split and compare the final test AUC and accuracy to those methods you tried in HW4.

**Answer:**

**SVM with polynomial kernel**

**Carseats dataset:**

```{r}
library(GGally)
library(gtsummary)
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

```

```{r}
# Numerical summaries
data(Carseats)
Carseats %>% tbl_summary()
```

```{r}
Carseats <- Carseats %>% filter(!is.na(Sales)) %>%
  mutate(Sales = ifelse(Sales <= 8, "Low", "High")) %>%
  print(width = 200)

```
**Initial split into test and non-test sets:**

```{r}
# For reproducibility
set.seed(212)

data_split <- initial_split(
  Carseats, 
  prop = 0.5,
  strata = Sales
  )
data_split

```

```{r}
Carseats_other <- training(data_split)
dim(Carseats_other)
```

```{r}
Carseats_test <- testing(data_split)
dim(Carseats_test)
```

**Recipe:**

```{r}
svm_poly_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_naomit(all_predictors()) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
svm_poly_recipe

```
**Model:**

```{r}
svm_poly_mod <- 
  # specify the model
  svm_poly(
    mode = "classification",
    engine = "kernlab",
    cost = tune(),
    degree = tune()
  ) %>%
  # specify the engine
  set_engine("kernlab") %>%
  # specify the resampling method
  set_mode("classification")

```

**Workflow:**

```{r}
svm_poly_wf <- workflow() %>%
  add_recipe(svm_poly_recipe) %>%
  add_model(svm_poly_mod) 
svm_poly_wf

```

**Tuning:**

```{r}
svm_poly_grid <- grid_regular(cost_complexity(),
                              degree(range = c(1, 3)))
                          

```

```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds

```


## Bonus (10pts)

Let
$$
f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = \beta_0 + \beta^T X. 
$$
Then $f(X)=0$ defines a hyperplane in $\mathbb{R}^p$. Show that $f(x)$ is proportional to the signed distance of a point $x$ to the hyperplane $f(X) = 0$. 

**Answer:**

The plane is 

$$
\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = 0

$$

The vector $\beta = (\beta_1, \beta2, ..., \beta_p)$ the normal to vector. It 
points in a direction orthogonal to the surface of hyperplane. 

Let

$$
\mathbf{v} = (\beta_1, \beta_2, ..., \beta_p)

$$

For any $x_0$ in the hyperplane $f(x)=0$, let

$$
x_0 = (x_{01}, x_{02}, ..., x_{0p})

$$

Therefore,

$$
\beta_0 + \beta_1 x_01 + \cdots + \beta_p x_0p = 0 \\
\Rightarrow -\beta_0 = \beta_1 x_01 + \cdots + \beta_p x_0p

$$

For any $x$, let

$$
x = (x_1, x_2, ..., x_p)
$$

The vector from $x_0$ to $x$ is

$$
\mathbf{w} = (x_1 - x_{01}, x_2 - x_{02}, ..., x_p - x_{0p})

$$

The the signed distance of $x$ is

$$
\begin{align*}
d &= \frac{\mathbf{v} \cdot \mathbf{w}}{\|\mathbf{v}\|} \\ 
&= \frac{\beta_1(x_1 - x_{01}) + \beta_2(x_2 - x_{02}) + \cdots + \beta_p(x_p - x_{0p})}{\sqrt{\beta_1^2 + \beta_2^2 + \cdots + \beta_p^2}} \\
&= \frac{\beta_1x_1  + \beta_2x_2  + \cdots + \beta_px_p - 
(\beta_1x_{01} + \beta_2x_{02} + \cdots + \beta_px_{0p} )}
{\sqrt{\beta_1^2 + \beta_2^2 + \cdots + \beta_p^2}} \\
&= \frac{\beta_1x_1  + \beta_2x_2  + \cdots + \beta_px_p + \beta_0 }{\|\beta\|} \\
&= \frac{\beta^T x + \beta_0}{\|\beta\|} \\
&= \frac{f(x)}{\|\beta\|} \\

\end{align*}
$$
Therefore, $f(x)$ is proportional to the signed distance of a point $x$ to the
hyperplane $f(X) = 0$.









