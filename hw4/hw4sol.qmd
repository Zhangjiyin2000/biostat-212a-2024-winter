---
title: "Biostat 212a Homework 4"
subtitle: "Due Mar. 5, 2024 @ 11:59PM"
author: "Jiyin (Jenny) Zhang, UID: 606331859"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 8.4.3 (10pts)

Consider the Gini index, classification error, and entropy in a simple 
classification setting with two classes. Create a single plot that displays each 
of these quantities as a function of $\hat p_{m1}$. The x-axis should display 
$\hat p_{m1}$, ranging from 0 to 1, and the $y$-axis should display the value of 
the Gini index, classification error, and entropy.

Hint: In a setting with two classes, $\hat p_{m1}$ = 1 âˆ’ $\hat p_{m2}$. You 
could make this plot by hand, but it will be much easier to make in R.

**Answer:**

Create a data frame with $\hat p_{m1}$ ranging from 0 to 1, and calculate the 
Gini index, classification error, and entropy. 

```{r}
library(ggplot2)
p <- seq(0, 1, 0.01)
df <- data.frame(
  p = p,
  gini = p * (1 - p) * 2,
  entropy = -(p * log(p) + (1 - p) * log(1 - p)),
  class_err = 1 - pmax(p, 1 - p)
) 
print(df, width = 200)
```

Melt the data frame for ggplot:

```{r}
library(reshape2)
df_melted <- melt(df, id.vars = "p") |>
  print(width = 200)
```

Then use `ggplot2` to plot the three quantities as a function of $\hat p_{m1}$.

```{r}
# Plot using ggplot
ggplot(df_melted, aes(x = p, y = value, color = variable), na.rm = true) +
  geom_line() +
  labs(
    title = "Gini index, classification error, and entropy",
    subtitle = "as a function of p_hat",
    x = expression(hat(p)),
    y = "Quantities",
    color = "Quantities"
  )
```

## ISL Exercise 8.4.4 (10pts)

This question relates to the plots in Figure 8.14.

![](figure8_14.png)

FIGURE 8.14. Left: A partition of the predictor space corresponding to Exercise
4a. Right: A tree corresponding to Exercise 4b.


__(a)__ Sketch the tree corresponding to the partition of the predictor
space illustrated in the left-hand panel of Figure 8.14. The numbers
inside the boxes indicate the mean of $Y$ within each region.

**Answer:**

![](q2_a.jpg)


__(b)__ Create a diagram similar to the left-hand panel of Figure 8.14,
using the tree illustrated in the right-hand panel of the same
figure. You should divide up the predictor space into the correct
regions, and indicate the mean for each region.

**Answer:**

![](q2_b.jpg)

## ISL Exercise 8.4.5 (10pts)

Suppose we produce ten bootstrapped samples from a data set containing red and 
green classes. We then apply a classification tree to each bootstrapped sample 
and, for a specific value of X, produce 10 estimates of $P$(Class is Red|X):

0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, and 0.75.

There are two common ways to combine these results together into a single class 
prediction. One is the majority vote approach discussed in this chapter. The 
second approach is to classify based on the average probability. In this 
example, what is the final classification under each of these two approaches?

**Answer:**

Generate the data:

```{r}
estimates <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75)
print(estimates)
num_estimates <- length(estimates)
cat('There are', num_estimates, 'estimates in total\n')
```

Assuming a decision threshold probability of 0.5

```{r}
p_threshold <- 0.5
numRed <- sum(estimates >= p_threshold)
numGreen <- sum(estimates < p_threshold)
```

For the majority vote approach:

```{r}
cat('Using the majority vote approach:\n')
cat('The number of votes for class Red =', numRed, '\n')
cat('The number of votes for class Green =', numGreen, '\n')
if (numRed >= numGreen) {
  cat('The assigned class is Red\n')
} else {
  cat('The assigned class is Green\n')
}
```
For the average probability approach:

```{r}
cat('Using the average vote approach:\n')
mean_estimate <- mean(estimates)
cat('The mean estimate is', mean_estimate, '\n')
if (mean_estimate >= p_threshold) {
  cat('The assigned class is Red\n')
} else {
  cat('The assigned class is Green\n')
}
```

Histogram with threshold, mean, and median lines:

```{r}
hist(estimates, breaks = 10, main = "Histogram of Estimates", xlab = "Estimates", col = "lightblue")
abline(v = p_threshold, col = "black", lwd = 2)
abline(v = mean_estimate, col = "red", lwd = 2)
abline(v = median(estimates), col = "blue", lwd = 2)
legend("topright", 
       legend = c("Threshold", "Mean", "Median"), 
       fill = c("black", "red", "blue"))

```


## ISL Lab 8.3. `Boston` data set (30pts)

Follow the machine learning workflow to train regression tree, random forest, 
and boosting methods for predicting `medv`. Evaluate out-of-sample performance 
on a test set.

**Answer:**

**Regression Tree**

**Boston dataset:**

```{r}
library(GGally)
library(gtsummary)
# install.packages("ranger")
library(ranger)
library(tidyverse)
library(tidymodels)
library(ISLR2)

# Numerical summaries 
Boston %>% tbl_summary()

```
```{r}
Boston <- Boston %>% filter(!is.na(medv)) %>%
  print(width = 200)

```
**Initial split into test and non-test sets:**

```{r}
# For reproducibility
set.seed(203)

data_split <- initial_split(
  Boston, 
  prop = 0.5
  )
data_split

```

```{r}
Boston_other <- training(data_split)
dim(Boston_other)
```
```{r}
Boston_test <- testing(data_split)
dim(Boston_test)
```

**Recipe:**

```{r}
tree_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
tree_recipe

```
**Model:**

```{r}
regtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "regression",
  engine = "rpart"
  ) 
```

**Workflow:**

```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(regtree_mod)
tree_wf
```
**Tuning grid:**

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100, 5))

```

**Cross-validation:**

```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds
```

Fit cross-validation:

```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(rmse, rsq)
    )
tree_fit

```

Visualize CV results:

```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV mse")

```
**Finalize our model:**

```{r}
tree_fit %>%
  show_best("rmse")

```
Select the best model.

```{r}
best_tree <- tree_fit %>%
  select_best("rmse")
best_tree

```

```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf

```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```
** Visualize the final model:**

```{r}
# install.packages("rpart.plot")
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

```
```{r}
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```
```{r}
# install.packages("vip")
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()

```
**random forest**

**Model:**

**Recipe**

```{r}
rf_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
rf_recipe

```

```{r}
rf_mod <- 
  rand_forest(
    mode = "regression",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

```
**Workflow:**

```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf
```
**Tuning grid:**

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid

```
**Cross-validation:**

```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds

```

Fit cross-validation:

```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
rf_fit

```

Visualize CV results:

```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV mse")

```
Show the top 5 models:

```{r}
rf_fit %>%
  show_best("rmse")
```
Select the best model:

```{r}
best_rf <- rf_fit %>%
  select_best("rmse")
best_rf
```
**Finalize our model:**

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf

```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()
```

**Boosting methods**

**Recipe:**

```{r}
gb_recipe <- 
  recipe(
    medv ~ ., 
    data = Boston_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  step_naomit(medv) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Boston_other, retain = TRUE)
gb_recipe

```
**Model:**

```{r}
gb_mod <- 
  boost_tree(
    mode = "regression",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod
```
**Workflow:**

```{r}
gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf

```
**Tuning grid:**

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 4L)),
  learn_rate(range = c(-3, -0.5), trans = log10_trans()),
  levels = c(4, 10)
  )
param_grid

```
**Cross-validation:**

```{r}
set.seed(203)

folds <- vfold_cv(Boston_other, v = 5)
folds
```

Fit cross-validation:

```{r}
# install.packages("xgboost")
library(xgboost)
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(rmse, rsq)
    )
gb_fit
```

Visualize CV results:

```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```
Show the top 5 models.

```{r}
gb_fit %>%
  show_best("rmse")

```
Select the best model.

```{r}
best_gb <- gb_fit %>%
  select_best("rmse")
best_gb

```
**Finalize our model:**

```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf

```
```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()

```

**Visualize the final model:**

```{r}
#library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()

```


## ISL Lab 8.3 `Carseats` data set (30pts)

Follow the machine learning workflow to train classification tree, random 
forest, and boosting methods for classifying `Sales <= 8` versus `Sales > 8`. 
Evaluate out-of-sample performance on a test set.

**Answer:**

**classification tree:**

**Carseats dataset:**

```{r}
# Numerical summaries
Carseats %>% tbl_summary()
```

```{r}
Carseats <- Carseats %>% filter(!is.na(Sales)) %>%
  mutate(Sales = ifelse(Sales <= 8, "Low", "High")) %>%
  print(width = 200)

```

**Initial split into test and non-test sets:**

```{r}
# For reproducibility
set.seed(212)

data_split <- initial_split(
  Carseats, 
  prop = 0.5,
  strata = Sales
  )
data_split

```

```{r}
Carseats_other <- training(data_split)
dim(Carseats_other)
```

```{r}
Carseats_test <- testing(data_split)
dim(Carseats_test)
```

**Recipe:**

```{r}
tree_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  step_naomit(all_predictors()) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
tree_recipe

```
**Model:**

```{r}
classtree_mod <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = 5,
  mode = "classification",
  engine = "rpart"
  ) 

```

**Workflow:**

```{r}
tree_wf <- workflow() %>%
  add_recipe(tree_recipe) %>%
  add_model(classtree_mod) 
tree_wf

```

**Tuning grid:**

```{r}
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = c(100,5))

```

**Cross-validation:**

```{r}
set.seed(212)

folds <- vfold_cv(Carseats_other, v = 5)
folds

```

Fit cross-validation:

```{r}
tree_fit <- tree_wf %>%
  tune_grid(
    resamples = folds,
    grid = tree_grid,
    metrics = metric_set(accuracy, roc_auc)
    )
tree_fit

```

Visualize CV results:

```{r}
tree_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = cost_complexity, y = mean, color = tree_depth)) +
  geom_point() + 
  geom_line() + 
  labs(x = "cost_complexity", y = "CV ROC AUC", color = "tree_depth") 

```

**Finalize our model:**

```{r}
tree_fit %>%
  show_best("roc_auc")

```

Select the best model.

```{r}
best_tree <- tree_fit %>%
  select_best("roc_auc")
best_tree

```

```{r}
# Final workflow
final_wf <- tree_wf %>%
  finalize_workflow(best_tree)
final_wf

```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()

```

** Visualize the final model:**

```{r}
library(rpart.plot)
final_tree <- extract_workflow(final_fit)
final_tree

```

```{r}
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```

```{r}
library(vip)

final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()

```

**random forest:**

**Recipe:**

```{r}
rf_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) %>%
  # mean imputation for Ca
  #step_impute_mean(Ca) %>%
  # mode imputation for Thal
  #step_impute_mode(Thal) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
rf_recipe

```
**Model:**

```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) %>% 
  set_engine("ranger")
rf_mod

```
**Workflow:**

```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_mod)
rf_wf

```
**Tuning grid:**

```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 300L)), 
  mtry(range = c(1L, 5L)),
  levels = c(3, 5)
  )
param_grid

```

**Cross-validation:**

```{r}
set.seed(203)

folds <- vfold_cv(Carseats_other, v = 5)
folds

```

Fit cross-validation:

```{r}
rf_fit <- rf_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
rf_fit

```

Visualize CV results:

```{r}
rf_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(mtry = as.factor(mtry)) %>%
  ggplot(mapping = aes(x = trees, y = mean, color = mtry)) +
  # geom_point() + 
  geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")

```

Show the top 5 models:

```{r}
rf_fit %>%
  show_best("roc_auc")

```

Select the best model:

```{r}
best_rf <- rf_fit %>%
  select_best("roc_auc")
best_rf

```
**Finalize our model:**

```{r}
# Final workflow
final_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_wf

```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```

```{r}
# Test metrics
final_fit %>% 
  collect_metrics()

```

**boosting methods**

**Recipe:**

```{r}
gb_recipe <- 
  recipe(
    Sales ~ ., 
    data = Carseats_other
  ) %>%
  # mean imputation for Ca
  #step_impute_mean(Ca) %>%
  # mode imputation for Thal
  #step_impute_mode(Thal) %>%
  # create traditional dummy variables (necessary for xgboost)
  step_dummy(all_nominal_predictors()) %>%
  # zero-variance filter
  step_zv(all_numeric_predictors()) %>% 
  # estimate the means and standard deviations
  prep(training = Carseats_other, retain = TRUE)
gb_recipe

```

**Model:**

```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 1000, 
    tree_depth = tune(),
    learn_rate = tune()
  ) %>% 
  set_engine("xgboost")
gb_mod

```
**Workflow:**

```{r}
gb_wf <- workflow() %>%
  add_recipe(gb_recipe) %>%
  add_model(gb_mod)
gb_wf
```

**Tuning grid:**

```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid
```

**Cross-validation:**

```{r}
set.seed(203)

folds <- vfold_cv(Carseats_other, v = 5)
folds
```

Fit cross-validation:

```{r}
gb_fit <- gb_wf %>%
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
    )
gb_fit
```

Visualize CV results:

```{r}
gb_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  mutate(tree_depth = as.factor(tree_depth)) %>%
  ggplot(mapping = aes(x = learn_rate, y = mean, color = tree_depth)) +
  geom_point() +
  geom_line() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()

```

Show the top 5 models.

```{r}
gb_fit %>%
  show_best("roc_auc")

```
Select the best model.

```{r}
best_gb <- gb_fit %>%
  select_best("roc_auc")
best_gb

```

**Finalize our model:**

```{r}
# Final workflow
final_wf <- gb_wf %>%
  finalize_workflow(best_gb)
final_wf

```

```{r}
# Fit the whole training set, then predict the test cases
final_fit <- 
  final_wf %>%
  last_fit(data_split)
final_fit

```


```{r}
# Test metrics
final_fit %>% 
  collect_metrics()

```



