---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "Jiyin (Jenny) Zhang, 606331859"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)

We will now derive the probability that a given observation is part of a 
bootstrap sample. Suppose that we obtain a bootstrap sample from a set of $n$ 
observations.

__(a)__ What is the probability that the first bootstrap observation is $not$ 
the $j$th observation from the original sample? Justify your
answer.

**Answer:** $1−1/n$

The probability that the first bootstrap observation is not the $j$th 
observation from the original sample is $1−1/n$. This is because there are $n$ 
samples, and we are equally likely to pick each observation when taking our 
first bootstrap observation.

__(b)__ What is the probability that the second bootstrap observation is not the 
$j$th observation from the original sample?

**Answer:** $1−1/n$

Since bootstrap observations are sampled from the original observations with 
replacement, the probability that the second bootstrap observation is not the 
$j$th observation from the original sample is again $1−1/n$.

__(c)__ Argue that the probability that the $j$th observation is $not$ in the
bootstrap sample is $(1 − 1/n)^n$.

**Answer:**

The probability that the $j$th observation is not in the bootstrap sample is
$(1 − 1/n)^n$. This is because the probability that the $j$th observation is
not in the bootstrap sample is the same as the probability that the first
bootstrap observation is not the $j$th observation from the original sample,
multiplied by the probability that the second bootstrap observation is not the
$j$th observation from the original sample, and so on, up to the $n$th
bootstrap observation. Since each of these probabilities is $(1 − 1/n)$, the
probability that the $j$th observation is not in the bootstrap sample is
$(1 − 1/n)^n$. Note that we are multiplying probabilities because sampling with 
replacement means that the individual bootstrap observations in the bootstrap 
sample are independent.



__(d)__ When $n$ = 5, what is the probability that the $j$th observation is in 
the bootstrap sample?

**Answer:**

When $n$ = 5, the probability that the $j$th observation is in the bootstrap
sample is $1−(1−1/5)^5 = 1−(4/5)^5 = 1−0.32768 = 0.67232$. 

__(e)__ When $n$ = 100, what is the probability that the $j$th observation is in 
the bootstrap sample?

**Answer:**

When $n$ = 100, the probability that the $j$th observation is in the bootstrap
sample is $1−(1−1/100)^100 = 1−(99/100)^100 = 1−0.36603 = 0.63397$.

__(f)__ When $n$ = 10,000, what is the probability that the $j$th observation is 
in the bootstrap sample?

**Answer:**

When $n$ = 10,000, the probability that the $j$th observation is in the
bootstrap sample is $1−(1−1/10000)^10000 = 1−(9999/10000)^10000 = 1−0.36769 = 0.63231$.

__(g)__ Create a plot that displays, for each integer value of $n$ from 1 to 
100,000, the probability that the $j$th observation is in the bootstrap sample. 
Comment on what you observe.

**Answer:**

```{r}
library(dplyr)
library(ggplot2)

n <- 1:10000

data.frame(n, prob_j_included = 1 - (1 - 1/n)^n) %>%
  ggplot(aes(x = n, y = prob_j_included)) + 
  geom_line(size = 1) + 
  geom_hline(aes(yintercept = 1 - 1/exp(1), col = "1 - 1/e")) +
  scale_color_manual(values = c("blue")) + 
  labs(title = "Bootstrap Sampling - Probability obs.j included", y = "Probability", col = "")

```

We are seeing a visual representation of 
$\lim_\limits{n\to \infty} {1−(1−\frac{1}{n})^n}=1−\frac1e≈0.632$, which is the 
reason why this is sometimes called the 0.632 rule in bootstrapping. This is the 
probability that the $j$th observation is in the bootstrap sample.

__(h)__ We will now investigate numerically the probability that a bootstrap
sample of size $n$ = 100 contains the $j$th observation. Here $j$ = 4. We 
repeatedly create bootstrap samples, and each time we record whether or not the 
fourth observation is contained in the bootstrap sample.

```{r}
store <- rep(NA, 10000)
for (i in 1:10000) {
  store[i] <- sum(sample(1:100, rep = TRUE) == 4) > 0
}
mean(store)
```

Comment on the results obtained.

**Answer:**

To relate this to previous questions, we created 10,000 separate bootstrap 
samples, sampling from the set of integers from 1 to 100: {1,2,…,100}.

In the `sample()` function there is a `size` argument, and since we did not set this, 
the default behavior is to sample n = 100 items from the vector x = 1:100. 
`replace = TRUE` means that this sampling occurs with replacement. This occurs 
for each $i$ in the loop.

`sum(sample(1:100, rep=TRUE) == 4) > 0` checks whether any of these 100 items 
were the number 4 (this is our $j$ in previous questions), returning TRUE if 4
was within the sample, else FALSE.

Assigning this to `store[i]` stores this result in the $i$th position of the 
vector store. This happens 10,000 times (one for each bootstrap sample), so the 
resulting store vector is logical with 10,000 entries.

At the end the mean is calculated, which for a logical vector will give the 
proportion of the 10,000 entries in store that had value `TRUE = 1`.

We create 10,000 bootstrap samples from {1,2,…,100}, then calculate what 
proportion of them contained 4.

Wwe would expect that if we perform many many bootstrap samples, around 63.2% of 
them will contain the number 4.

The simulation shows that this seems to hold true, with 63.2% containing 4.

## ISL Exercise 5.4.9 (20pts)

We will now consider the Boston housing data set, from the ISLR2 library.

__(a)__ Based on this data set, provide an estimate for the population
mean of `medv`. Call this estimate $\hat\mu$.

**Answer:**

```{r}
library(ISLR2)
data("Boston")
head(Boston)
mu_hat <- mean(Boston$medv)
cat("The population mean of medv is:", mu_hat, "\n")
```

__(b)__ Provide an estimate of the standard error of $\hat\mu$. Interpret this
result.
Hint: We can compute the standard error of the sample mean by dividing the 
sample standard deviation by the square root of the
number of observations.

**Answer:**

```{r}
se_mu_hat <- sd(Boston$medv) / sqrt(length(Boston$medv))
cat(
  "The estimate of the standard error of the population mean of medv is:",
  se_mu_hat,
  "\n"
)
```

__(c)__ Now estimate the standard error of $\hat\mu$ using the bootstrap. How
does this compare to your answer from (b)?

**Answer:**

```{r}
# Install the boot package if you haven't already
# install.packages("boot")
# Load the boot package
library(boot)
```

```{r}
set.seed(1)
boot.fn <- function(data, index) {
  mu <- mean(data[index])
  return(mu)
}
boot(Boston$medv, boot.fn, 1000)
```

The bootstrap estimated standard error of $\hat\mu$ is very close to the 
estimate found in (b).

__(d)__ Based on your bootstrap estimate from (c), provide a 95 % confidence
interval for the mean of medv. Compare it to the results obtained using 
t.test(Boston$medv).
Hint: You can approximate a 95 % confidence interval using the formula 
$[\hat\mu - 2SE(\hat\mu), \hat\mu + 2SE(\hat\mu)]$.

**Answer:**

```{r}
# Calculate the 95% confidence interval using the formula
standard_error <- 0.4106622
ci_lower <- mu_hat - 2 * standard_error
ci_upper <- mu_hat + 2 * standard_error
cat(
  "The 95% confidence interval for the mean of medv is: (",
  ci_lower,
  ",",
  ci_upper,
  ")\n"
)
```

```{r}
# Compare it to the results obtained using t.test(Boston$medv)
t.test(Boston$medv)
```
The bootstrap confidence interval is very close to the one provided by the 
`t.test()` function.

__(e)__ Based on this data set, provide an estimate, $\hat\mu_{med}$, for the 
median value of `medv` in the population.

**Answer:**

```{r}
mu_med_hat <- median(Boston$medv)
cat(
  "The estimate for the median value of medv in the population is:",
  mu_med_hat,
  "\n"
)
```

__(f)__ We now would like to estimate the standard error of $\hat\mu_{med}$. 
Unfortunately, there is no simple formula for computing the standard error of 
the median. Instead, estimate the standard error of the median using the 
bootstrap. Comment on your findings.

**Answer:**

```{r}
set.seed(1)
boot.fn <- function(data, index) {
  mu_med <- median(data[index])
  return(mu_med)
}
boot(Boston$medv, boot.fn, 1000)
```

We get an estimated median value of 21.2 which is equal to the value obtained in 
(e), with a standard error of 0.3778075 which is relatively small compared to 
median value.

__(g)__ Based on this data set, provide an estimate for the tenth percentile
of `medv` in Boston census tracts. Call this quantity $\hat\mu_{0.1}$. (You can 
use the quantile() function.)

**Answer:**

```{r}
mu_0.1_hat <- quantile(Boston$medv, 0.1)
cat(
  "The estimate for the tenth percentile of medv in the population is:",
  mu_0.1_hat,
  "\n"
)
```

__(h)__ Use the bootstrap to estimate the standard error of $\hat\mu_{0.1}$. 
Comment on your findings.

**Answer:**

```{r}
set.seed(1)
boot.fn <- function(data, index) {
  mu_0.1 <- quantile(data[index], 0.1)
  return(mu_0.1)
}
boot(Boston$medv, boot.fn, 1000)
```

We get an estimated tenth percentile value of 12.75 which is again equal to the 
value obtained in (g), with a standard error of 0.4767526 which is relatively 
small compared to the tenth percentile value.


## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

**Answer:**

The linear model with Gaussian errors is given by

$$
Y = X\beta + \epsilon
$$

where $\epsilon \sim N(0, \sigma^2)$.

The likelihood function is given by

$$
L(\beta, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y - X\beta)^T*(Y - X\beta)}{2\sigma^2}\right)
$$

The log-likelihood function is given by

$$
\ell(\beta, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)
$$

Compute the derivative of $\beta$ with respect to $(Y - X\beta)^T*(Y - X\beta) $

$$
\begin{align*}
\frac{\partial}{\partial\beta} (Y - X\beta)^T*(Y - X\beta) 
&= \frac{\partial}{\partial\beta} (Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta) \\
&= \frac{\partial}{\partial\beta} (Y^TY - 2\beta^TX^TY - \beta^TX^TX\beta) \\
&= -2 X^T Y +2 X^T X\beta \\
&= -2 X^T (Y - X \beta) \\
\end{align*}
$$

Then compute the derivative of $\beta$ with respect to $l(\beta,\sigma^2)$

$$
\begin{align*}
\frac{\partial}{\partial\beta} \ell(\beta, \sigma^2) 
&= \frac{\partial}{\partial\beta} \left(-\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) \\
&= \frac{\partial}{\partial\beta} \left(-\frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) \\
&= \frac{1}{2\sigma^2} 2 X^T (Y - X \beta) \\
\end{align*}
$$

Set the derivative of $\beta$ with respect to $l(\beta,\sigma^2)$ to zero

$$
\begin{align*}
\frac{1}{2\sigma^2} 2 X^T (Y - X \beta) &= 0 \\
X^T (Y - X \beta) &= 0 \\
X^T Y - X^T X \beta &= 0 \\
X^T Y = X^T X \beta \\
\end{align*}
$$

Solve for $\beta$

$$
\hat\beta = (X^TX)^{-1}X^TY
$$

and as for $\sigma^2$

$$
\sigma^2 = \frac{1}{n} (Y - X\beta)^2
$$

The least squares estimates are given by

$$
\hat\beta = (X^TX)^{-1}X^TY
$$

Therefore, maximum likelihood and least squares are the same thing.

Here is $C_p$

$$
\begin{align*}
C_p &= \frac{1}{n} \left(\text{RSS} + 2d\hat\sigma^2\right) \\
\end{align*}
$$
Assume that 

$$
\hat\sigma^2 = \sigma^2
$$

Therefore, 

$$
C_p = \frac{1}{n} \left(\text{RSS} + 2d\sigma^2\right) 
$$

and here is AIC

$$
\begin{align*}
AIC &= -2logL + 2d \\
&= -2\left(-\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) + 2d \\
&= n\log(2\pi) + n\log(\sigma^2) +\frac{1}{\sigma^2} (Y - X\beta)^T*(Y - X\beta) + 2d \\
&= n\log(2\pi) + n\log(\sigma^2) +\frac{1}{\sigma^2} RSS + 2d \\
\end{align*}
$$

where $\text{RSS} = (Y - X\beta)^T*(Y - X\beta)$ and $d$ is the number of predictors.

Since $n\log(2\pi)$ and $n\log(\sigma^2)$ are constants and thus the model selection depends only on the term $\frac{1}{\sigma^2} RSS$ and $2d$. We ingore the constant terms and rewrite AIC:

$$
AIC \approx  \frac{1}{\sigma^2} RSS + 2d
$$

Multiply $AIC$ by $\frac{\sigma^2}{n}$:

$$
\frac{\sigma^2}{n} * AIC = \frac{\sigma^2}{n} (\frac{1}{\sigma^2} RSS + 2d)= \frac{1}{n} (RSS + 2d\sigma^2) = C_p

$$

Therefore, $C_p$ and AIC are equivalent in the function of model selection.


## ISL Exercise 6.6.1 (10pts)
We perform best subset, forward stepwise, and backward stepwise selection on a 
single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, 
..., p predictors. Explain your answers:

__(a)__ Which of the three models with k predictors has the smallest training 
RSS?

**Answer:**

The model with best subset selection has the smallest training RSS since it 
considers every possible model with k predictors. 

__(b)__ Which of the three models with k predictors has the smallest test RSS?

**Answer:**

Providing an accurate assessment is challenging based on the information 
available. Best subset may have the smallest test RSS because it considers more 
models. However when n is relatively small compared to p, the best subset might 
experience overfitting. Additionally, the other two methods could accidentally 
select a model that performs better on the test set.

__(c)__ True or False:

i. The predictors in the k-variable model identified by forward stepwise are a 
subset of the predictors in the (k+1)-variable model identified by forward 
stepwise selection.

**Answer:**

TRUE

In each step, forward stepwise augments with one additional predictor, hence the 
statement is true.

ii. The predictors in the k-variable model identified by backward stepwise are a 
subset of the predictors in the (k + 1)-variable model identified by backward 
stepwise selection.

**Answer:**

TRUE

In each step the backward stepwise the least useful predictor is removed, hence 
the answer is true.

iii. The predictors in the k-variable model identified by backward stepwise are 
a subset of the predictors in the (k + 1)-variable model identified by forward 
stepwise selection.

**Answer:**

FALSE

Forward and backward stepwise are independent functions, thus the above 
statement is false.


iv. The predictors in the k-variable model identified by forward stepwise are a 
subset of the predictors in the (k+1)-variable model identified by backward 
stepwise selection.

**Answer:**

FALSE

Forward and backward stepwise are independent functions, thus the above 
statement is false.


v. The predictors in the k-variable model identified by best subset are a subset 
of the predictors in the (k + 1)-variable model identified by best subset 
selection.

**Answer:**

FALSE

Best subset chooses the best model for each k, therefore the above statement is 
false.

## ISL Exercise 6.6.3 (10pts)

Suppose we estimate the regression coefficients in a linear regression
model by minimizing

$\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2$ subject to 
$\sum_{j=1}^{p}|\beta_j| \le s$

for a particular value of $s$. For parts (a) through (e), indicate which
of i. through v. is correct. Justify your answer.

__(a)__ As we increase s from 0, the training RSS will:

i. Increase initially, and then eventually start decreasing in an inverted U shape.

ii. Decrease initially, and then eventually start increasing in a U shape.

iii. Steadily increase.

iv. Steadily decrease.

v. Remain constant.

**Answer:** iv. Steadily decrease.

When s=0, we have the intercept, which is the expected mean value. As s→∞, we 
end up with a standard least squares. We should see a decrease in the bias and 
therefore a steady decrease in the training RSS as we approach a least squares 
regression.

__(b)__ Repeat (a) for test RSS.

**Answer:** ii. Decrease initially, and then eventually start increasing in a U shape.

When s=0, we have a very high test RSS. As s increases we see the model start to 
fit the test data and thus the test RSS decreases. However there is a point 
where the variance starts to increase faster than the bias is decreasing and it 
starts to overfit the training data. At this point the test RSS will start to 
increase again.

__(c)__ Repeat (a) for variance.

**Answer:** iii. Steadily increase.

When s=0 the variance is very low. As s increases, the variance steadily 
increases as the model starts to fit the training data.

__(d)__ Repeat (a) for (squared) bias.

**Answer:** iv. Steadily decrease.

When s=0, the bias is very high, and as s increases the bias steadily decreases 
as the model fits the training data better.


__(e)__ Repeat (a) for the irreducible error.

**Answer:** v. Remain constant.

The irreducible error is a constant and does not change as s increases and it is 
by definition independent of the model. Therefore it is independent of the value 
of s.

## ISL Exercise 6.6.4 (10pts)

Suppose we estimate the regression coefficients in a linear regression model by 
minimizing

$$
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + 
\lambda\sum_{j=1}^{p}\beta_j^2

$$

for a particular value of !. For parts (a) through (e), indicate which of i. 
through v. is correct. Justify your answer.

__(a)__ As we increase $\lambda$ from 0, the training RSS will:

i. Increase initially, and then eventually start decreasing in an inverted U shape.

ii. Decrease initially, and then eventually start increasing in a U shape.

iii. Steadily increase.

iv. Steadily decrease.

v. Remain constant.

**Answer:** iii. Steadily increase.

When $\lambda = 0$, we have the standard partial least squares. As λ→∞, we are 
applying a shrinking penalty which is small when βj is close to zero and 
therefore shrinks $\beta_j$ close to zero. As $\lambda$ increases, the bias 
increases and thus we should see a steady increase in the training RSS.

__(b)__ Repeat (a) for test RSS.

**Answer:** ii. Decrease initially, and then eventually start increasing in a U shape.

As we increase $\lambda$, we'll see an increase in bias but a decrease in 
variance up until a point where the coefficients are shrunk too. At the end of 
the scale when $\lambda\rightarrow\infty$, we approach the null model. Therefore 
we should see an initial decrease in the test RSS up to a point, then an 
increase in the test RSS.


__(c)__ Repeat (a) for variance.

**Answer:** iv. Steadily decrease.

As we constrain the coefficients we see the variance steadily decrease.


__(d)__ Repeat (a) for (squared) bias.

**Answer:** iii. Steadily increase.

As we constrain the coefficients we see a steady increase in the bias.

__(e)__ Repeat (a) for the irreducible error.

**Answer:** v. Remain constant.

The irreducible error is independent of the model and therefore is a constant.

## ISL Exercise 6.6.5 (10pts)

It is well-known that ridge regression tends to give similar coefficient values 
to correlated variables, whereas the lasso may give quite different coefficient 
values to correlated variables. We will now explore this property in a very 
simple setting.

Suppose that n = 2, p = 2, $x_{11} = x_{12}$, $x_{21} = x_{22}$. Furthermore,
suppose that $y_1 + y_2 = 0$ and $x_{11} + x_{21} = 0$ and $x_{12} + x_{22} =0$, 
so that the estimate for the intercept in a least squares, ridge regression, or
lasso model is zero: $\hat\beta_0 = 0$.

__(a)__ Write out the ridge regression optimization problem in this setting.

**Answer:**

In general, the ridge regression optimization problem is given by

$$
min[\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}\beta_j^2]
$$

In this case, $\hat\beta_0 = 0$ and $n=p=2$. So, the optimization simplifies to

$$
min[(y_1 - \hat\beta_1 x_{11} - \hat\beta_2 x_{12})^2 + (y_2 - \hat\beta_1 x_{21} - \hat\beta_2 x_{22})^2 + \lambda( \hat\beta_1^2 + \hat\beta_2^2)]
$$

We know the following: $x_{11} = x_{12}$, so we'll call that $x_1$, and 
$x_{21} = x_{22}$, so we'll call that $x_2$. Plugging in the values we know, we
get 

$$
min[(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1)^2 + (y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2)^2 + \lambda( \hat\beta_1^2 + \hat\beta_2^2)]
$$

__(b)__ Argue that in this setting, the ridge coefficient estimates satisfy 
$\hat\beta_1=\hat\beta_2$.

**Answer:**

Taking the partial derivatives of the ridge regression optimization problem with
respect to $\hat\beta_1$ and $\hat\beta_2$ and setting them equal to zero, we
get

$$
\frac{\partial}{\partial\hat\beta_1} = 0 \rightarrow -2x_1(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1) - 2x_2(y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2) + 2\lambda\hat\beta_1 = 0
$$

$$
 -2x_1(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1) - 2x_2(y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2) = - 2\lambda\hat\beta_1 
$$

and

$$
\frac{\partial}{\partial\hat\beta_2} = 0 \rightarrow -2x_1(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1) - 2x_2(y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2) + 2\lambda\hat\beta_2 = 0
$$

$$
-2x_1(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1) - 2x_2(y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2) = - 2\lambda \hat\beta_2
$$

Since the left side of the equations are the same, we can assume the right sides 
equal each other, so

$$
- 2\lambda\hat\beta_1 = - 2\lambda\hat\beta_2
$$

Thus, we can conclude that in this setting, the ridge coefficient estimates

$$
\hat\beta_1 = \hat\beta_2
$$


__(c)__ Write out the lasso optimization problem in this setting.

**Answer:**

The lasso optimization problem is given by

$$
min[\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})^2 + \lambda\sum_{j=1}^{p}|\beta_j|]
$$

In this case, $\hat\beta_0 = 0$ and $n=p=2$. So, the optimization simplifies to

$$
min[(y_1 - \hat\beta_1 x_{11} - \hat\beta_2 x_{12})^2 + (y_2 - \hat\beta_1 x_{21} - \hat\beta_2 x_{22})^2 + \lambda( |\hat\beta_1| + |\hat\beta_2|)]
$$

We know the following: $x_{11} = x_{12}$, so we'll call that $x_1$, and
$x_{21} = x_{22}$, so we'll call that $x_2$. Plugging in the values we know, we
get

$$
min[(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1)^2 + (y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2)^2 + \lambda( |\hat\beta_1| + |\hat\beta_2|)]
$$

__(d)__ Argue that in this setting, the lasso coefficients $\hat\beta_1$ and 
$\hat\beta_2$ are not unique—in other words, there are many possible solutions
to the optimization problem in (c). Describe these solutions.

**Answer:**

The lasso optimization problem is given by

$$
min[(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1)^2 + (y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2)^2 + \lambda( |\hat\beta_1| + |\hat\beta_2|)]
$$

One way to demonstrate that these solutions are not unique is to make a 
geometric argument. To make things easier, we’ll use the alternate form of Lasso 
constraints, namely: $|\hat\beta_1| + |\hat\beta_2| < s$. If we were to plot 
these constraints, they take the familiar shape of a diamond centered at the 
origin (0, 0).

Next we’ll consider the squared optimization constraint, namely:

$$
(y_1 - \hat\beta_1 x_1 - \hat\beta_2 x_1)^2 + (y_2 - \hat\beta_1 x_2 - \hat\beta_2 x_2)^2
$$xuan

Using the facts we were given regarding the equivalence of many of the 
variables, we can simplify down to the following optimization:

$$
min\ 2[y_1 - (\hat\beta_1 + \hat\beta_2 )x_1]^2
$$

Taking the derivative of this with respect to $\hat\beta_1 + \hat\beta_2$ and
setting it equal to zero, we get

$$
\frac{\partial}{\partial(\hat\beta_1 + \hat\beta_2)} = 0 \rightarrow 2[y_1 - (\hat\beta_1 + \hat\beta_2 )x_1](-x_1) = 0
$$

$$
y_1 - (\hat\beta_1 + \hat\beta_2 )x_1 = 0
$$

$$
\hat\beta_1 + \hat\beta_2 = y_1/x_1
$$

This optimization problem has a minimum at $\hat\beta_1 +\hat\beta_2 = y_1/x_1$, 
which defines a line parallel to one edge of the Lasso-diamond 
$\hat\beta_1 + \hat\beta_2 =s$ or $\hat\beta_1 + \hat\beta_2 = -s$.

As $\hat\beta_1$ and $\hat\beta_2$ vary along the line 
$\hat\beta_1 +\hat\beta_2 = y_1/x_1$, these contours touch the Lasso-diamond
edge $\hat\beta_1 + \hat\beta_2 =s$ or $\hat\beta_1 + \hat\beta_2 = -s$ at 
different points. As a result, the entire edge $\hat\beta_1 + \hat\beta_2 =s$ or 
$\hat\beta_1 + \hat\beta_2 = -s$ is a potential solution to the Lasso 
optimization problem.

Thus, the Lasso coefficients are not unique. The general form of solution can be given by
two line segments:

$$
\hat\beta_1 + \hat\beta_2 = s; \hat\beta_1 \ge 0; \hat\beta_2 \ge 0; \hat\beta_1 + \hat\beta_2 = -s; \hat\beta_1 \le 0; \hat\beta_2 \le 0
$$


## ISL Exercise 6.6.11 (30pts)

We will now try to predict per capita crime rate in the Boston data set.

__(a)__ Try out some of the regression methods explored in this chapter, such as 
best subset selection, the lasso, ridge regression, and PCR. Present and discuss 
results for the approaches that you consider.

**Answer:**

You must follow the [typical machine learning paradigm](https://ucla-econ-425t.github.io/2023winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

**Answer:**

**Explore the data:**

```{r}
library(ISLR2)
data("Boston")
library(tidymodels)
library(tidyverse)
boston <- as_tibble(Boston) %>% print(width = Inf)
```

```{r}
# Numerical summaries
summary(boston)
```

```{r}
library(GGally)
# Graphical summaries
ggpairs(
  data = boston,
  mapping = aes(alpha = 0.25),
  lower = list(continuous = "smooth")
) +
  labs(title = "Boston Data")
```

```{r}
boston <- boston %>%
  drop_na()
dim(boston)
```

**Initial split into test and non-test sets:**

```{r}
# For reproducibility
set.seed(425)
data_split <- initial_split(
  boston,
  prop = 0.75
)

boston_other <- training(data_split)
dim(boston_other)
```

```{r}
boston_test <- testing(data_split)
dim(boston_test)
```

**Recipe (R):**

```{r}
norm_recipe <-
  recipe(
    crim ~ .,
    data = boston_other
  ) %>%
  # create traditional dummy variables
  step_dummy(all_nominal()) %>%
  # zero-variance filter
  step_zv(all_predictors()) %>%
  # center and scale numeric data
  step_normalize(all_predictors()) %>%
  # estimate the means and standard deviations
  prep(training = boston_other, retain = TRUE)
norm_recipe
```

**Model:**

Least squares:
```{r}
lm_mod <-
  linear_reg() %>%
  set_engine("lm")
lm_mod
```
Ridge:
```{r}
ridge_mod <-
  # mixture = 0 (ridge), mixture = 1 (lasso)
  linear_reg(penalty = tune(), mixture = 0) %>%
  set_engine("glmnet")
ridge_mod
```
Lasso:
```{r}
lasso_mod <-
  # mixture = 0 (ridge), mixture = 1 (lasso)
  linear_reg(penalty = tune(), mixture = 1.0) %>%
  set_engine("glmnet")
lasso_mod
```


**Workflow (R):**

Least squares:
```{r}
ls_lr_wf <-
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(norm_recipe)
ls_lr_wf
```
Ridge:
```{r}
ridge_lr_wf <-
  workflow() %>%
  add_model(ridge_mod) %>%
  add_recipe(norm_recipe)
ridge_lr_wf
```
Lasso:
```{r}
lasso_lr_wf <-
  workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(norm_recipe)
lasso_lr_wf
```

**Tuning grid:**

```{r}
lambda_grid <-
  grid_regular(penalty(range = c(-2, 3), trans = log10_trans()), levels = 100)
lambda_grid
```

**Cross-validation (CV):**

```{r}
set.seed(250)
folds <- vfold_cv(boston_other, v = 10)
folds
```

**Fit cross-validation:**

least squares:
```{r}
# Fit the cross-validation
cv_results <- ls_lr_wf %>%
  fit_resamples(resamples = folds, metrics = metric_set(rmse))

# Summarize the results
summary(cv_results)

```

Ridge:
```{r}
ridge_fit <-
  ridge_lr_wf %>%
  tune_grid(
    resamples = folds,
    grid = lambda_grid
  )
ridge_fit
```

Lasso:
```{r}
lasso_fit <-
  lasso_lr_wf %>%
  tune_grid(
    resamples = folds,
    grid = lambda_grid
  )
lasso_fit
```

**Visualize CV criterion:**

Least squares:
```{r}
# Collect cross-validation metrics
cv_metrics <- collect_metrics(cv_results)
cv_metrics
cv_rmse_ls <- cv_metrics$mean
cv_rmse_ls
```

Ridge:
```{r}
ridge_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Penalty", y = "CV RMSE") +
  scale_x_log10(labels = scales::label_number())
```

```{r}
cv_rmse_ridge <- ridge_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  pull(mean)
cv_rmse_ridge
```
Lasso:
```{r}
lasso_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  ggplot(mapping = aes(x = penalty, y = mean)) +
  geom_point() +
  geom_line() +
  labs(x = "Penalty", y = "CV RMSE") +
  scale_x_log10(labels = scales::label_number())
```

```{r}
cv_rmse_lasso <- lasso_fit %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "rmse") %>%
  pull(mean)
cv_rmse_lasso
```

**Show the top 5 models ($\lambda$values):**


Ridge:
```{r}
ridge_fit %>%
  show_best("rmse")
```
Lasso:
```{r}
lasso_fit %>%
  show_best("rmse")
```

**Select the best model:**

Ridge:
```{r}
best_ridge <- ridge_fit %>%
  select_best("rmse")
best_ridge
```
Lasso:
```{r}
best_lasso <- lasso_fit %>%
  select_best("rmse")
best_lasso
```

**Finalize our model:**

Final workflow:

Ridge:
```{r}
ridge_final_wf <- ridge_lr_wf %>%
  finalize_workflow(best_ridge)
ridge_final_wf
```

Lasso:
```{r}
lasso_final_wf <- lasso_lr_wf %>%
  finalize_workflow(best_lasso)
lasso_final_wf
```

**Fit the whole training set, then predict the test cases**

Least squares:
```{r}
ls_final_fit <-
  ls_lr_wf %>%
  last_fit(data_split)

```

Ridge:
```{r}
ridge_final_fit <-
  ridge_final_wf %>%
  last_fit(data_split)
ridge_final_fit
```
Lasso:
```{r}
lasso_final_fit <-
  lasso_final_wf %>%
  last_fit(data_split)
lasso_final_fit
```
**Test metrics:**

Least square:
```{r}
ls_final_fit %>% collect_metrics()
test_rmse_ls <- ls_final_fit %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)
test_rmse_ls
```

Ridge:
```{r}
ridge_final_fit %>% collect_metrics()
test_rmse_ridge <- ridge_final_fit %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)
test_rmse_ridge
```
Lasso:
```{r}
lasso_final_fit %>% collect_metrics()
test_rmse_lasso <- lasso_final_fit %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(.estimate)
test_rmse_lasso
```

```{r}
# Create a tibble to store the results
results_df <- tibble(
  Method = c("LS", "Ridge", "Lasso"),
  CV_RMSE = c(cv_rmse_ls, mean(cv_rmse_ridge), mean(cv_rmse_lasso)),
  Test_RMSE = c(test_rmse_ls, test_rmse_ridge, test_rmse_lasso)
)

# Print the final results
print(results_df)
```

__(b)__ Propose a model (or set of models) that seem to perform well on this 
data set, and justify your answer. Make sure that you are evaluating model 
performance using validation set error, crossvalidation, or some other 
reasonable alternative, as opposed to using training error.

**Answer:**

Based on the results of test RMSE, the Ridge model seems to perform the best on 
this data set since it has the lowest test RMSE. Because the Ridge model uses a
penalty term to shrink the coefficients of the features towards zero, it is
better at handling multicollinearity than the Lasso model. The Lasso model
performs the worst on this data set, which is likely due to the fact that it
selects a subset of the features and sets the coefficients of the other features
to zero. The least squares model performs better than the Lasso model, but not as
well as the Ridge model. This is likely due to the fact that the least squares
model does not use a penalty term to shrink the coefficients of the features
towards zero, and therefore is not as good at handling multicollinearity as the
Ridge model.

__(c)__ Does your chosen model involve all of the features in the data set? Why 
or why not?

**Answer:**

The chosen model does not involve all of the features in the data set. The Ridge
model involves a subset of the features in the data set. This is because the
Ridge model uses a penalty term to shrink the coefficients of the features
towards zero.

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that

$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$
**Answer:**

Since the training data and test data are drawn at random from the same 
population. Therefore, we have

$$
min_\beta\  \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2 = min\  \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2
$$

Thus, we have

$$
\hat \beta = \tilde \beta
$$
Therefore, we have

$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] = \operatorname{E}[R_{\text{test}}(\tilde{\beta})].
$$
When we replace $\tilde\beta$ with $\hat\beta$, we cannot get the minimum value. 
Therefore, we have

$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].

$$






