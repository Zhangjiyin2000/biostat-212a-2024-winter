---
title: "Biostat 212A Homework 3"
subtitle: "Due Feb 20, 2024 @ 11:59PM"
author: "Jiyin (Jenny) Zhang, 606331859"
date: today
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: true
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
engine: knitr
knitr:
  opts_chunk: 
    fig.align: 'center'
    # fig.width: 6
    # fig.height: 4
    message: FALSE
    cache: false
---

## ISL Exercise 5.4.2 (10pts)
Suppose that we use some statistical learning method to make a prediction for 
the response Y for a particular value of the predictor X. Carefully describe 
how we might estimate the standard deviation of our prediction.

### Answer:
We can estimate the standard deviation of our prediction by using the bootstrap 
method. We can obtain repeated random samples from the original data set without 
generating additional samples. To elucidate, we perform sampling with replacement 
B times and then find the corresponding estimates and the standard deviation of 
those B estimates by using the following equation:
$$
SE_B(\hat\alpha) = \sqrt{\frac{1}{B-1} \sum_{r=1}^B (\hat\alpha^{*r} - \frac{1}{B} \sum_{r=1}^B \hat\alpha^{*r'})^2}

$$
where $\hat\alpha^{*r}$ is the estimate of $\hat\alpha$ for the $r^{th}$ 
bootstrap sample and $\frac{1}{B} \sum_{r=1}^B \hat\alpha^{*r'}$ is the average 
of the B estimates.

## ISL Exercise 5.4.9 (20pts)
We will now consider the Boston housing data set, from the ISLR2 library.
__(a)__ Based on this data set, provide an estimate for the population
mean of `medv`. Call this estimate $\hat\mu$.
### Answer:
```{r}
library(ISLR2)
data("Boston")
head(Boston)
mu_hat <- mean(Boston$medv)
cat("The population mean of medv is:", mu_hat, "\n")
```

__(b)__ Provide an estimate of the standard error of $\hat\mu$. Interpret this
result.
Hint: We can compute the standard error of the sample mean by dividing the 
sample standard deviation by the square root of the
number of observations.
### Answer:
```{r}
se_mu_hat <- sd(Boston$medv) / sqrt(length(Boston$medv))
cat("The estimate of the standard error of the population mean of medv is:", 
    se_mu_hat, 
    "\n")
```

__(c)__ Now estimate the standard error of $\hat\mu$ using the bootstrap. How
does this compare to your answer from (b)?
### Answer:
```{r}
# Install the boot package if you haven't already
# install.packages("boot")
# Load the boot package
library(boot)
```

```{r}
set.seed(1)
boot.fn <- function(data, index) {
    mu <- mean(data[index])
    return (mu)
}
boot(Boston$medv, boot.fn, 1000)
```

The bootstrap estimated standard error of $\hat\mu$ is very close to the 
estimate found in (b).

__(d)__ Based on your bootstrap estimate from (c), provide a 95 % confidence
interval for the mean of medv. Compare it to the results obtained using 
t.test(Boston$medv).
Hint: You can approximate a 95 % confidence interval using the formula 
$[\hat\mu - 2SE(\hat\mu), \hat\mu + 2SE(\hat\mu)]$.
### Answer:
```{r}
# Calculate the 95% confidence interval using the formula
ci_lower <- mu_hat - 2 * standard_error
ci_upper <- mu_hat + 2 * standard_error
cat("The 95% confidence interval for the mean of medv is: (", 
    ci_lower, 
    ",", 
    ci_upper, 
    ")\n")
```

```{r}
# Compare it to the results obtained using t.test(Boston$medv)
t.test(Boston$medv)
```
The bootstrap confidence interval is very close to the one provided by the 
`t.test()` function.

__(e)__ Based on this data set, provide an estimate, $\hat\mu_{med}$, for the 
median value of `medv` in the population.
### Answer:
```{r}
mu_med_hat <- median(Boston$medv)
cat("The estimate for the median value of medv in the population is:", 
    mu_med_hat, 
    "\n")
```

__(f)__ We now would like to estimate the standard error of $\hat\mu_{med}$. 
Unfortunately, there is no simple formula for computing the standard error of 
the median. Instead, estimate the standard error of the median using the 
bootstrap. Comment on your findings.
### Answer:
```{r}
set.seed(1)
boot.fn <- function(data, index) {
    mu_med <- median(data[index])
    return (mu_med)
}
boot(Boston$medv, boot.fn, 1000)
```
We get an estimated median value of 21.2 which is equal to the value obtained in 
(e), with a standard error of 0.3778075 which is relatively small compared to 
median value.

__(g)__ Based on this data set, provide an estimate for the tenth percentile
of `medv` in Boston census tracts. Call this quantity $\hat\mu_{0.1}$. (You can 
use the quantile() function.)
### Answer:
```{r}
mu_0.1_hat <- quantile(Boston$medv, 0.1)
cat("The estimate for the tenth percentile of medv in the population is:", 
    mu_0.1_hat, 
    "\n")
```

__(h)__ Use the bootstrap to estimate the standard error of $\hat\mu_{0.1}$. 
Comment on your findings.
### Answer:
```{r}
set.seed(1)
boot.fn <- function(data, index) {
    mu_0.1 <- quantile(data[index], 0.1)
    return (mu_0.1)
}
boot(Boston$medv, boot.fn, 1000)
```

We get an estimated tenth percentile value of 12.75 which is again equal to the 
value obtained in (g), with a standard error of 0.4767526 which is relatively 
small compared to the tenth percentile value.


## Least squares is MLE (10pts)

Show that in the case of linear model with Gaussian errors, maximum likelihood and least squares are the same thing, and $C_p$ and AIC are equivalent.

### Answer:
The linear model with Gaussian errors is given by
$$
Y = X\beta + \epsilon
$$
where $\epsilon \sim N(0, \sigma^2)$.

The likelihood function is given by
$$
L(\beta, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(Y - X\beta)^T*(Y - X\beta)}{2\sigma^2}\right)
$$

The log-likelihood function is given by
$$
\ell(\beta, \sigma^2) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)
$$

Compute the derivative of $\beta$ with respect to $(Y - X\beta)^T*(Y - X\beta) $
$$
\begin{align*}
\frac{\partial}{\partial\beta} (Y - X\beta)^T*(Y - X\beta) 
&= \frac{\partial}{\partial\beta} (Y^TY - Y^TX\beta - \beta^TX^TY + \beta^TX^TX\beta) \\
&= \frac{\partial}{\partial\beta} (Y^TY - 2\beta^TX^TY - \beta^TX^TX\beta) \\
&= -2 X^T Y +2 X^T X\beta \\
&= -2 X^T (Y - X \beta) \\

\end{align*}
$$
Then compute the derivative of $\beta$ with respect to $l(\beta,\sigma^2)$
$$
\begin{align*}
\frac{\partial}{\partial\beta} \ell(\beta, \sigma^2) 
&= \frac{\partial}{\partial\beta} \left(-\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) \\
&= \frac{\partial}{\partial\beta} \left(-\frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) \\
&= \frac{1}{2\sigma^2} 2 X^T (Y - X \beta) \\

\end{align*}
$$
Set the derivative of $\beta$ with respect to $l(\beta,\sigma^2)$ to zero
$$
\begin{align*}
\frac{1}{2\sigma^2} 2 X^T (Y - X \beta) &= 0 \\
X^T (Y - X \beta) &= 0 \\
X^T Y - X^T X \beta &= 0 \\
X^T Y = X^T X \beta \\

\end{align*}
$$
Solve for $\beta$
$$
\hat\beta = (X^TX)^{-1}X^TY
$$
and as for $\sigma^2$
$$
\sigma^2 = \frac{1}{n} (Y - X\beta)^2
$$

The least squares estimates are given by
$$
\hat\beta = (X^TX)^{-1}X^TY
$$
Therefore, maximum likelihood and least squares are the same thing.

Here is  $C_p$
$$
C_p = \frac{1}{n} \left(\text{RSS} + 2d\hat\sigma^2\right)
$$
and here is AIC
$$
\begin{align*}
AIC &= -2logL + 2d \\
&= -2\left(-\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} (Y - X\beta)^T*(Y - X\beta)\right) + 2d \\
&= n\log(2\pi) + n\log(\sigma^2) +\frac{1}{\sigma^2} (Y - X\beta)^T*(Y - X\beta) + 2d \\
\end{align*}
$$
where $\text{RSS} = \sum_{i=1}^n (y_i - x_i^T\hat\beta)^2$ and $d$ is the number of predictors.

Therefore, $C_p$ and AIC are equivalent.


## ISL Exercise 6.6.1 (10pts)

## ISL Exercise 6.6.3 (10pts)

## ISL Exercise 6.6.4 (10pts)

## ISL Exercise 6.6.5 (10pts)

## ISL Exercise 6.6.11 (30pts)

You must follow the [typical machine learning paradigm](https://ucla-econ-425t.github.io/2023winter/slides/06-modelselection/workflow_lasso.html) to compare _at least_ 3 methods: least squares, lasso, and ridge. Report final results as

| Method | CV RMSE | Test RMSE |
|:------:|:------:|:------:|:------:|
| LS | | | |
| Ridge | | | |
| Lasso | | | |
| ... | | | |

## Bonus question (20pts)

Consider a linear regression, fit by least squares to a set of training data $(x_1, y_1), \ldots, (x_N,  y_N)$ drawn at random from a population. Let $\hat \beta$ be the least squares estimate. Suppose we have some test data $(\tilde{x}_1, \tilde{y}_1), \ldots, (\tilde{x}_M, \tilde{y}_M)$ drawn at random from the same population as the training data. If $R_{\text{train}}(\beta) = \frac{1}{N} \sum_{i=1}^N (y_i - \beta^T x_i)^2$ and $R_{\text{test}}(\beta) = \frac{1}{M} \sum_{i=1}^M (\tilde{y}_i - \beta^T \tilde{x}_i)^2$. Show that
$$
\operatorname{E}[R_{\text{train}}(\hat{\beta})] < \operatorname{E}[R_{\text{test}}(\hat{\beta})].
$$